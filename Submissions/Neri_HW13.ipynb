{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hello and welcome to:\n",
    "### Patrick's Jupyter Notebook!!\n",
    "\n",
    "This is a proof of concept done using the non-map parts of my groups forcast program."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'dataretrieval'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-0081d34678d9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mdataretrieval\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnwis\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnwis\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'dataretrieval'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import dataretrieval.nwis as nwis\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn import metrics\n",
    "import json \n",
    "import urllib.request as req\n",
    "import urllib\n",
    "import eval_functions\n",
    "import contextily as ctx\n",
    "from shapely.geometry import Point\n",
    "import geopandas as gpd\n",
    "import fiona\n",
    "import matplotlib as mpl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets get some functions real quick"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getstrm_wbs(station_id,end_date):\n",
    "    \"\"\"Get stream flow from \n",
    "    https://waterdata.usgs.gov/nwis/.\n",
    "    ---------------------------------\n",
    "    This function download streamflow. It needs \n",
    "    as input the station id number and the end date of data.\n",
    "    Dataset start on 1989-01-01.\n",
    "    ---------------------------------\n",
    "    Parameters:\n",
    "    station_id = list of string numbers\n",
    "    end_date = string date as yyyy-mm-dd\n",
    "    ----------------------------------\n",
    "    Outputs:\n",
    "    lastNames = dataframe with streamflow values and dates\n",
    "    \"\"\"\n",
    "    start_date = '1989-01-01'\n",
    "    flow_data = nwis.get_record(sites=station_id, service='dv',\n",
    "                          start=start_date, end=end_date,\n",
    "                          parameterCd='00060')\n",
    "    flow_data.columns = ['flow', 'code', 'site_no']\n",
    "    flow_data = flow_data.rename_axis(\"datetime\")\n",
    "    flow_data['datetime'] = pd.to_datetime(flow_data.index)\n",
    "    return(flow_data)\n",
    "\n",
    "def add_yymmdd(flow_data):\n",
    "    \"\"\"Add year,week,day columns to data \n",
    "    ---------------------------------\n",
    "    This function adds year,week,day \n",
    "    colummns to data to facilitate computation\n",
    "    ---------------------------------\n",
    "    Parameters:\n",
    "    data = dataframe data\n",
    "    ----------------------------------\n",
    "    Outputs:\n",
    "    flow_data = dataframe with extra columns\n",
    "    \"\"\"\n",
    "\n",
    "    #flow_data['datetime'] = pd.to_datetime(flow_data.index)\n",
    "    flow_data['datetime'] = pd.to_datetime(flow_data.index)\n",
    "    flow_data['year'] = pd.DatetimeIndex(flow_data.index).year\n",
    "    flow_data['month'] = pd.DatetimeIndex(flow_data.index).month\n",
    "    flow_data['day'] = pd.DatetimeIndex(flow_data.index).day\n",
    "    flow_data['dayofweek'] = pd.DatetimeIndex(flow_data.index).dayofweek\n",
    "    return(flow_data)\n",
    "\n",
    "def mono_reg_mod(test_weeks):\n",
    "    \"\"\"Linear Regression Model data being offset only once.\n",
    "    test weeks = natural log streamflow laged by 1 week (x values)\n",
    "    test weeks = natural log streamflow (y values)\n",
    "    \"\"\"\n",
    "    reg_model = LinearRegression()\n",
    "    x_val_model1 = test_weeks['log_flow_tm1'].values.reshape(-1, 1)  # Testing values\n",
    "    y_val_model1 = test_weeks['log_flow'].values  # Testing values\n",
    "    reg_model.fit(x_val_model1, y_val_model1)  # Fit linear model\n",
    "    coeff_det1 = np.round(reg_model.score(x_val_model1, y_val_model1), 7)  # r^2\n",
    "    b = np.round(reg_model.intercept_, 7)  # Intercept\n",
    "    m = np.round(reg_model.coef_, 7)  # Slope\n",
    "    print('coefficient of determination:', np.round(coeff_det1, 7))\n",
    "    # Intercept and the slope (Final equation) y= mx + b\n",
    "    print('Final equation is y1 = :', m[:1], 'x + ', b)\n",
    "    return(b,m,reg_model,coeff_det1)\n",
    "\n",
    "def flow_predic_mono(b, m, num_of_weeks, week_b4, forecast_weeks):\n",
    "    \"\"\"This function produces predicted flow values using coefficients provided\n",
    "    by an Liner Autoregressive Model with only one data offset.\n",
    "    'b' is the y-intersept and 'm' is the slope.\n",
    "    'num_of_weeks' is how many weeks you would like to loop the model for.\n",
    "    'week_b4' is the natural log flow of a known flow and\n",
    "    'forecast_weeks' is a list of dates that you are predicting for.\n",
    "    \"\"\"\n",
    "    week_b4_i = week_b4\n",
    "    pred_i = np.zeros((num_of_weeks, 1))\n",
    "    for i in range(1, num_of_weeks + 1):\n",
    "            log_flow_pred_i = b + m[:1] * week_b4_i\n",
    "            flow_pred_i = math.exp(log_flow_pred_i)\n",
    "            pred_i[i-1] = flow_pred_i\n",
    "            week_b4_i = log_flow_pred_i\n",
    "    flow_predictions_lin = pd.DataFrame(pred_i, index = forecast_weeks,\n",
    "                                        columns=[\"Predicted_Flows_Lin:\"])\n",
    "    return flow_predictions_lin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### First up is defining the timeframe for training period and retrieving the streamflow values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "station_id = '09506000'  # Streamflow station\n",
    "trainstart = '2016-01-01'  # Start date to train AR model\n",
    "trainend = '2019-12-31'  # end date to train AR model\n",
    "lag = 2  # No. of weeks to consider for lag \n",
    "end_date = '2020-11-21'  # yyyy-mm-dd (changes each week)\n",
    "flow_data = getstrm_wbs(station_id,end_date)  # get strmflow data from website\n",
    "flow_data_pd = add_yymmdd(flow_data)  # add year,month,day\n",
    "flow_weekly = flow_data_pd.resample(\"W\", on='datetime').mean()  # Add flow values to weekly\n",
    "flow_weekly.insert(2, 'log_flow', np.log(flow_weekly['flow']), True)  # Natural log (fits the model better)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Next set up the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shifts = list(range(1, lag+1))\n",
    "flow_weekly['log_flow_tm1'] = flow_weekly['log_flow'].shift(shifts[0])  # Lag 1week\n",
    "flow_weekly['log_flow_tm2'] = flow_weekly['log_flow'].shift(shifts[1])  # Lag 2weeks\n",
    "print('Start training week: ', trainstart)\n",
    "print('End training week: ', trainend)\n",
    "train = flow_weekly[trainstart:trainend][['log_flow',\n",
    "                                          'log_flow_tm1', 'log_flow_tm2']]\n",
    "test = flow_weekly[trainend:][['log_flow',\n",
    "                               'log_flow_tm1', 'log_flow_tm2']]\n",
    "b, m, reg_model1, coeff_det1 = mono_reg_mod(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next up we want our 2 week prediction! So let's run a little code and..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "week_before_flow = flow_weekly['log_flow'].tail(1)\n",
    "print(\"Last weeks's flow was\", math.exp(week_before_flow),'cfs!', '\\n')\n",
    "forecast_week_1_2 = ['2020-11-09','2020-11-16']\n",
    "print(flow_predic_mono(b, m, 2, week_before_flow, forecast_week_1_2), '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Nice, we have Numbers!!\n",
    "But that is only our 2 week, so Let's quick get our 16 week too!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_weeks = flow_weekly[\"log_flow\"].size  # Number of weeks up to date\n",
    "begining_week_ly = 25  # start week year 2020\n",
    "ending_week_ly = 12  # end week year 2020\n",
    "dates_weeks_range = flow_weekly['log_flow'][no_weeks-begining_week_ly:\n",
    "                                           no_weeks-ending_week_ly] \n",
    "\n",
    "wk_prd = np.zeros(16)\n",
    "for i in range(1,17):\n",
    "       wk_prd = week_prediction_all(flow=flow_weekly, m=m, b=b,\n",
    "                                    prev_wks=begining_week_ly, end=ending_week_ly, week_pred=i)\n",
    "       begining_week_ly = begining_week_ly+1\n",
    "       ending_week_ly = ending_week_ly +1\n",
    "print(wk_prd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And that is that! But why not include...\n",
    "### FUN GRAPHS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'flow_data_pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-b4cb6dbaaeef>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdata_mnth\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mflow_data_pd\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mflow_data_pd\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'month'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m7\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mflow_weekly_mnth\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mflow_weekly\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mflow_weekly\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'month'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m7\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mflow_quants_mnth\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mquantile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mflow_weekly_mnth\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'flow'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mq\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0.5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0.75\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0.9\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Method of flow quantiles for month '\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata_mnth\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m':'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mflow_quants_mnth\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'For plots, Green is flow max above 75%, and Red is below 50%'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'flow_data_pd' is not defined"
     ]
    }
   ],
   "source": [
    "data_mnth = flow_data_pd[flow_data_pd['month'] > 7]\n",
    "flow_weekly_mnth = flow_weekly[flow_weekly['month'] > 7]\n",
    "flow_quants_mnth = np.quantile(flow_weekly_mnth['flow'], q=[0, 0.5, 0.75, 0.9])\n",
    "print('Method of flow quantiles for month ', data_mnth, ':', flow_quants_mnth)\n",
    "print('For plots, Green is flow max above 75%, and Red is below 50%')\n",
    "fig = plt.figure(figsize=(30, 10))\n",
    "fig.subplots_adjust(hspace=0.6, wspace=0.4)\n",
    "\n",
    "for i in range(1, 31):\n",
    "    curr_yr = (i + 1990)\n",
    "    flow_weekly_mnth_i = flow_weekly_mnth[flow_weekly_mnth['year'] ==\n",
    "                                          curr_yr]\n",
    "    data_mnth_i = data_mnth[data_mnth['year'] == curr_yr]\n",
    "    ax = fig.add_subplot(3, 10, i)\n",
    "    ax.set(title=(\"Streamflow in \" + str(curr_yr)),\n",
    "           ylabel=\"Weekly Avg Flow [cfs]\", yscale='log')\n",
    "    plt.xticks(rotation=45)\n",
    "    if (np.max(flow_weekly_mnth_i['flow']) > flow_quants_mnth[2]):\n",
    "        ax.plot(flow_weekly_mnth_i['flow'],\n",
    "                '-g', label='Weekly Average')\n",
    "        ax.plot(data_mnth_i['datetime'], data_mnth_i['flow'], color='grey',\n",
    "                label='Daily Flow')\n",
    "        ax.legend()\n",
    "    elif (np.max(flow_weekly_mnth_i['flow']) < flow_quants_mnth[1]):\n",
    "        ax.plot(flow_weekly_mnth_i['flow'],\n",
    "                '-r', label='Weekly Average')\n",
    "        ax.plot(data_mnth_i['datetime'], data_mnth_i['flow'], color='grey',\n",
    "                label='Daily Flow')\n",
    "        ax.legend()\n",
    "    else:\n",
    "        ax.plot(flow_weekly_mnth_i['flow'],\n",
    "                '-b', label='Weekly Average')\n",
    "        ax.plot(data_mnth_i['datetime'], data_mnth_i['flow'], color='grey', \n",
    "                label='Daily Flow')\n",
    "        ax.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:hastools2] *",
   "language": "python",
   "name": "conda-env-hastools2-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
